{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# w205 Project 2: Joshua Archer\n",
    "## Contents\n",
    "\n",
    "(1) Publish and consume messages with kafka\n",
    "\n",
    "(2) Use Spark to transform messages\n",
    "\n",
    "(3) Check out the json data\n",
    "\n",
    "(4) Answer business questions\n",
    "\n",
    "(5) Write to HDFS in parquet format for all created dataframes\n",
    "\n",
    "(6) Relevant history from the console\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pprint\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## (1) Publish and consume messages with kafka\n",
    "\n",
    "#### Copy in old yaml file\n",
    "\n",
    "```cp ~/w205/course-content/06-Transforming-Data/docker-compose.yml ~/w205/project-2-jearcher/```\n",
    "\n",
    "#### Copy in new YAML file (week 07)\n",
    "\n",
    "```cp ~/w205/course-content/07-Sourcing-Data/docker-compose.yml ~/w205/project-2-jearcher/```\n",
    "\n",
    "#### Make sure in right directory \n",
    "\n",
    "```cd ~/w205/project-2-jearcher/```\n",
    "\n",
    "#### Spin up cluster\n",
    "\n",
    "```docker-compose up -d```\n",
    "\n",
    "#### Start the log file in 2nd command line (kafka mirror)\n",
    "\n",
    "```docker-compose logs -f kafka```\n",
    "\n",
    "#### Create a kafka topic called 'assessments'\n",
    "\n",
    "```docker-compose exec kafka kafka-topics --create --topic assessments --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181```\n",
    "\n",
    "#### Check/describe the topic\n",
    "\n",
    "```docker-compose exec kafka kafka-topics --describe --topic assessments --zookeeper zookeeper:32181```\n",
    "\n",
    "#### Publish messages to topic with kafka console producer (Change filename)\n",
    "\n",
    "```docker-compose exec mids bash -c \"cat /w205/project-2-jearcher/assessment-attempts-20180128-121051-nested.json | jq '.[]' -c | kafkacat -P -b kafka:29092 -t assessments \"```\n",
    "\n",
    "#### Consume messages:\n",
    "\n",
    "```docker-compose exec mids bash -c \"kafkacat -C -b kafka:29092 -t assessments -o beginning -e\"```\n",
    "\n",
    "\n",
    "Alternate way to consume messages:\n",
    "\n",
    "```docker-compose exec kafka kafka-console-consumer --bootstrap-server localhost:29092 --topic assessments --from-beginning --max-messages 3280```\n",
    "\n",
    "#### Shut down cluster\n",
    "\n",
    "```docker-compose down```\n",
    "\n",
    "#### Check for any strays\n",
    "\n",
    "```docker ps -a```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (2) Use PySpark to transform messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read from kafka using pyspark, subscribe to kafka topic and create a dataframe\n",
    "assessments = spark.read.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"kafka:29092\").option(\"subscribe\",\"assessments\").option(\"startingOffsets\", \"earliest\").option(\"endingOffsets\", \"latest\").load() \n",
    "\n",
    "# Check out the schema\n",
    "assessments.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "| key|               value|\n",
      "+----+--------------------+\n",
      "|null|{\"keen_timestamp\"...|\n",
      "|null|{\"keen_timestamp\"...|\n",
      "|null|{\"keen_timestamp\"...|\n",
      "|null|{\"keen_timestamp\"...|\n",
      "|null|{\"keen_timestamp\"...|\n",
      "|null|{\"keen_timestamp\"...|\n",
      "|null|{\"keen_timestamp\"...|\n",
      "|null|{\"keen_timestamp\"...|\n",
      "|null|{\"keen_timestamp\"...|\n",
      "|null|{\"keen_timestamp\"...|\n",
      "|null|{\"keen_timestamp\"...|\n",
      "|null|{\"keen_timestamp\"...|\n",
      "|null|{\"keen_timestamp\"...|\n",
      "|null|{\"keen_timestamp\"...|\n",
      "|null|{\"keen_timestamp\"...|\n",
      "|null|{\"keen_timestamp\"...|\n",
      "|null|{\"keen_timestamp\"...|\n",
      "|null|{\"keen_timestamp\"...|\n",
      "|null|{\"keen_timestamp\"...|\n",
      "|null|{\"keen_timestamp\"...|\n",
      "+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- key: string (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3280"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cast assessments as strings, store json converted to string in a new dataframe\n",
    "assessments_as_strings=assessments.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "\n",
    "# Check it out\n",
    "assessments_as_strings.show()\n",
    "assessments_as_strings.printSchema()\n",
    "assessments_as_strings.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Check out / Unroll the json data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start by checking out the json data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = pprint.PrettyPrinter(indent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open(\"assessment-attempts-20180128-121051-nested.json\",\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json_data = json.loads(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3280"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out how many assessments there are\n",
    "len(json_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'base_exam_id': '37f0a30a-7464-11e6-aa92-a8667f27e5dc',\n",
      " 'certification': 'false',\n",
      " 'exam_name': 'Normal Forms and All That Jazz Master Class',\n",
      " 'keen_created_at': '1516717442.735266',\n",
      " 'keen_id': '5a6745820eb8ab00016be1f1',\n",
      " 'keen_timestamp': '1516717442.735266',\n",
      " 'max_attempts': '1.0',\n",
      " 'sequences': {'attempt': 1,\n",
      "               'counts': {'all_correct': False,\n",
      "                          'correct': 2,\n",
      "                          'incomplete': 1,\n",
      "                          'incorrect': 1,\n",
      "                          'submitted': 4,\n",
      "                          'total': 4,\n",
      "                          'unanswered': 0},\n",
      "               'id': '5b28a462-7a3b-42e0-b508-09f3906d1703',\n",
      "               'questions': [{'id': '7a2ed6d3-f492-49b3-b8aa-d080a8aad986',\n",
      "                              'options': [{'at': '2018-01-23T14:23:24.670Z',\n",
      "                                           'checked': True,\n",
      "                                           'correct': True,\n",
      "                                           'id': '49c574b4-5c82-4ffd-9bd1-c3358faf850d',\n",
      "                                           'submitted': 1},\n",
      "                                          {'at': '2018-01-23T14:23:25.914Z',\n",
      "                                           'checked': True,\n",
      "                                           'correct': True,\n",
      "                                           'id': 'f2528210-35c3-4320-acf3-9056567ea19f',\n",
      "                                           'submitted': 1},\n",
      "                                          {'checked': False,\n",
      "                                           'correct': True,\n",
      "                                           'id': 'd1bf026f-554f-4543-bdd2-54dcf105b826'}],\n",
      "                              'user_correct': False,\n",
      "                              'user_incomplete': True,\n",
      "                              'user_result': 'missed_some',\n",
      "                              'user_submitted': True},\n",
      "                             {'id': 'bbed4358-999d-4462-9596-bad5173a6ecb',\n",
      "                              'options': [{'at': '2018-01-23T14:23:30.116Z',\n",
      "                                           'checked': True,\n",
      "                                           'id': 'a35d0e80-8c49-415d-b8cb-c21a02627e2b',\n",
      "                                           'submitted': 1},\n",
      "                                          {'checked': False,\n",
      "                                           'correct': True,\n",
      "                                           'id': 'bccd6e2e-2cef-4c72-8bfa-317db0ac48bb'},\n",
      "                                          {'at': '2018-01-23T14:23:41.791Z',\n",
      "                                           'checked': True,\n",
      "                                           'correct': True,\n",
      "                                           'id': '7e0b639a-2ef8-4604-b7eb-5018bd81a91b',\n",
      "                                           'submitted': 1}],\n",
      "                              'user_correct': False,\n",
      "                              'user_incomplete': False,\n",
      "                              'user_result': 'incorrect',\n",
      "                              'user_submitted': True},\n",
      "                             {'id': 'e6ad8644-96b1-4617-b37b-a263dded202c',\n",
      "                              'options': [{'at': '2018-01-23T14:23:52.510Z',\n",
      "                                           'checked': False,\n",
      "                                           'id': 'a9333679-de9d-41ff-bb3d-b239d6b95732'},\n",
      "                                          {'checked': False,\n",
      "                                           'id': '85795acc-b4b1-4510-bd6e-41648a3553c9'},\n",
      "                                          {'at': '2018-01-23T14:23:54.223Z',\n",
      "                                           'checked': True,\n",
      "                                           'correct': True,\n",
      "                                           'id': 'c185ecdb-48fb-4edb-ae4e-0204ac7a0909',\n",
      "                                           'submitted': 1},\n",
      "                                          {'at': '2018-01-23T14:23:53.862Z',\n",
      "                                           'checked': True,\n",
      "                                           'correct': True,\n",
      "                                           'id': '77a66c83-d001-45cd-9a5a-6bba8eb7389e',\n",
      "                                           'submitted': 1}],\n",
      "                              'user_correct': True,\n",
      "                              'user_incomplete': False,\n",
      "                              'user_result': 'correct',\n",
      "                              'user_submitted': True},\n",
      "                             {'id': '95194331-ac43-454e-83de-ea8913067055',\n",
      "                              'options': [{'checked': False,\n",
      "                                           'id': '59b9fc4b-f239-4850-b1f9-912d1fd3ca13'},\n",
      "                                          {'checked': False,\n",
      "                                           'id': '2c29e8e8-d4a8-406e-9cdf-de28ec5890fe'},\n",
      "                                          {'checked': False,\n",
      "                                           'id': '62feee6e-9b76-4123-bd9e-c0b35126b1f1'},\n",
      "                                          {'at': '2018-01-23T14:24:00.807Z',\n",
      "                                           'checked': True,\n",
      "                                           'correct': True,\n",
      "                                           'id': '7f13df9c-fcbe-4424-914f-2206f106765c',\n",
      "                                           'submitted': 1}],\n",
      "                              'user_correct': True,\n",
      "                              'user_incomplete': False,\n",
      "                              'user_result': 'correct',\n",
      "                              'user_submitted': True}]},\n",
      " 'started_at': '2018-01-23T14:23:19.082Z',\n",
      " 'user_exam_id': '6d4089e4-bde5-4a22-b65f-18bce9ab79c8'}\n"
     ]
    }
   ],
   "source": [
    "# Pretty print json to look at it with human eyes and brain\n",
    "# Helps to see all the different data stored here\n",
    "p.pprint(json_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             keen_id|\n",
      "+--------------------+\n",
      "|5a6745820eb8ab000...|\n",
      "|5a674541ab6b0a000...|\n",
      "|5a67999d3ed3e3000...|\n",
      "|5a6799694fc7c7000...|\n",
      "|5a6791e824fccd000...|\n",
      "|5a67a0b6852c2a000...|\n",
      "|5a67b627cc80e6000...|\n",
      "|5a67ac8cb0a5f4000...|\n",
      "|5a67a9ba060087000...|\n",
      "|5a67ac54411aed000...|\n",
      "+--------------------+\n",
      "\n",
      "+------------------+-------------------------------------------------------+\n",
      "|    keen_timestamp|sequences[questions] AS `questions`[0][user_incomplete]|\n",
      "+------------------+-------------------------------------------------------+\n",
      "| 1516717442.735266|                                                   true|\n",
      "| 1516717377.639827|                                                  false|\n",
      "| 1516738973.653394|                                                  false|\n",
      "|1516738921.1137421|                                                  false|\n",
      "| 1516737000.212122|                                                  false|\n",
      "| 1516740790.309757|                                                  false|\n",
      "|1516746279.3801291|                                                  false|\n",
      "| 1516743820.305464|                                                  false|\n",
      "|  1516743098.56811|                                                  false|\n",
      "| 1516743764.813107|                                                  false|\n",
      "+------------------+-------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Unroll json !! If Error: RDD EMPTY, make sure you published on kafka (part 1 of this markdown)\n",
    "raw_assessments = spark.read.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"kafka:29092\").option(\"subscribe\",\"assessments\").option(\"startingOffsets\", \"earliest\").option(\"endingOffsets\", \"latest\").load() \n",
    "\n",
    "raw_assessments.cache()\n",
    "\n",
    "assessments = raw_assessments.select(raw_assessments.value.cast('string'))\n",
    "\n",
    "extracted_assessments = assessments.rdd.map(lambda x: Row(**json.loads(x.value))).toDF()\n",
    "\n",
    "extracted_assessments.registerTempTable('assessments')\n",
    "\n",
    "spark.sql(\"select keen_id from assessments limit 10\").show()\n",
    "\n",
    "spark.sql(\"select keen_timestamp, sequences.questions[0].user_incomplete from assessments limit 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "assessments = raw_assessments.select(raw_assessments.value.cast('string'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple query to check if it is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           questions|\n",
      "+--------------------+\n",
      "|[Map(user_incompl...|\n",
      "|[Map(user_incompl...|\n",
      "|[Map(user_incompl...|\n",
      "|[Map(user_incompl...|\n",
      "|[Map(user_incompl...|\n",
      "|[Map(user_incompl...|\n",
      "|[Map(user_incompl...|\n",
      "|[Map(user_incompl...|\n",
      "|[Map(user_incompl...|\n",
      "|[Map(user_incompl...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select sequences.questions from assessments limit 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'questions': [{'user_incomplete': True, 'user_correct': False, 'options': [{'checked': True, 'at': '2018-01-23T14:23:24.670Z', 'id': '49c574b4-5c82-4ffd-9bd1-c3358faf850d', 'submitted': 1, 'correct': True}, {'checked': True, 'at': '2018-01-23T14:23:25.914Z', 'id': 'f2528210-35c3-4320-acf3-9056567ea19f', 'submitted': 1, 'correct': True}, {'checked': False, 'correct': True, 'id': 'd1bf026f-554f-4543-bdd2-54dcf105b826'}], 'user_submitted': True, 'id': '7a2ed6d3-f492-49b3-b8aa-d080a8aad986', 'user_result': 'missed_some'}, {'user_incomplete': False, 'user_correct': False, 'options': [{'checked': True, 'at': '2018-01-23T14:23:30.116Z', 'id': 'a35d0e80-8c49-415d-b8cb-c21a02627e2b', 'submitted': 1}, {'checked': False, 'correct': True, 'id': 'bccd6e2e-2cef-4c72-8bfa-317db0ac48bb'}, {'checked': True, 'at': '2018-01-23T14:23:41.791Z', 'id': '7e0b639a-2ef8-4604-b7eb-5018bd81a91b', 'submitted': 1, 'correct': True}], 'user_submitted': True, 'id': 'bbed4358-999d-4462-9596-bad5173a6ecb', 'user_result': 'incorrect'}, {'user_incomplete': False, 'user_correct': True, 'options': [{'checked': False, 'at': '2018-01-23T14:23:52.510Z', 'id': 'a9333679-de9d-41ff-bb3d-b239d6b95732'}, {'checked': False, 'id': '85795acc-b4b1-4510-bd6e-41648a3553c9'}, {'checked': True, 'at': '2018-01-23T14:23:54.223Z', 'id': 'c185ecdb-48fb-4edb-ae4e-0204ac7a0909', 'submitted': 1, 'correct': True}, {'checked': True, 'at': '2018-01-23T14:23:53.862Z', 'id': '77a66c83-d001-45cd-9a5a-6bba8eb7389e', 'submitted': 1, 'correct': True}], 'user_submitted': True, 'id': 'e6ad8644-96b1-4617-b37b-a263dded202c', 'user_result': 'correct'}, {'user_incomplete': False, 'user_correct': True, 'options': [{'checked': False, 'id': '59b9fc4b-f239-4850-b1f9-912d1fd3ca13'}, {'checked': False, 'id': '2c29e8e8-d4a8-406e-9cdf-de28ec5890fe'}, {'checked': False, 'id': '62feee6e-9b76-4123-bd9e-c0b35126b1f1'}, {'checked': True, 'at': '2018-01-23T14:24:00.807Z', 'id': '7f13df9c-fcbe-4424-914f-2206f106765c', 'submitted': 1, 'correct': True}], 'user_submitted': True, 'id': '95194331-ac43-454e-83de-ea8913067055', 'user_result': 'correct'}], 'attempt': 1, 'id': '5b28a462-7a3b-42e0-b508-09f3906d1703', 'counts': {'incomplete': 1, 'submitted': 4, 'incorrect': 1, 'all_correct': False, 'correct': 2, 'total': 4, 'unanswered': 0}}\n"
     ]
    }
   ],
   "source": [
    "assessments_as_strings.select('value').take(1)\n",
    "\n",
    "assessments_as_strings.select('value').take(1)[0].value\n",
    "\n",
    "\n",
    "\n",
    "first_assessment=json.loads(assessments_as_strings.select('value').take(1)[0].value)\n",
    "\n",
    "first_assessment\n",
    "\n",
    "print(first_assessment['sequences'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create lambda transform to start creating dataframes, temptables and queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|        sequences_id|\n",
      "+--------------------+\n",
      "|5b28a462-7a3b-42e...|\n",
      "|5b28a462-7a3b-42e...|\n",
      "|b370a3aa-bf9e-4c1...|\n",
      "|b370a3aa-bf9e-4c1...|\n",
      "|04a192c1-4f5c-4ac...|\n",
      "|e7110aed-0d08-4cb...|\n",
      "|5251db24-2a6e-424...|\n",
      "|066b5326-e547-4da...|\n",
      "|8ac691f8-8c1a-403...|\n",
      "|066b5326-e547-4da...|\n",
      "+--------------------+\n",
      "\n",
      "+--------------------+------------------+--------------------+\n",
      "|             keen_id|    keen_timestamp|        sequences_id|\n",
      "+--------------------+------------------+--------------------+\n",
      "|5a17a67efa1257000...|1511499390.3836269|8ac691f8-8c1a-403...|\n",
      "|5a26ee9cbf5ce1000...|1512500892.4166169|9bd87823-4508-4e0...|\n",
      "|5a29dcac74b662000...|1512692908.8423469|e7110aed-0d08-4cb...|\n",
      "|5a2fdab0eabeda000...|1513085616.2275269|cd800e92-afc3-447...|\n",
      "|5a30105020e9d4000...|1513099344.8624721|8ac691f8-8c1a-403...|\n",
      "|5a3a6fc3f0a100000...| 1513779139.354213|e7110aed-0d08-4cb...|\n",
      "|5a4e17fe08a892000...|1515067390.1336551|9abd5b51-6bd8-11e...|\n",
      "|5a4f3c69cc6444000...| 1515142249.858722|083844c5-772f-48d...|\n",
      "|5a51b21bd0480b000...| 1515303451.773272|e7110aed-0d08-4cb...|\n",
      "|5a575a85329e1a000...| 1515674245.348099|25ca21fe-4dbb-446...|\n",
      "+--------------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# extract sequence.id with lambda transform\n",
    "def my_lambda_sequences_id(x):\n",
    "    raw_dict = json.loads(x.value)\n",
    "    my_dict = {\"keen_id\" : raw_dict[\"keen_id\"], \"sequences_id\" : raw_dict[\"sequences\"][\"id\"]}\n",
    "    return Row(**my_dict)\n",
    "\n",
    "# create separate dataframe\n",
    "my_sequences = assessments.rdd.map(my_lambda_sequences_id).toDF()\n",
    "\n",
    "# Register as a temp table for SQL\n",
    "my_sequences.registerTempTable('sequences')\n",
    "\n",
    "# Do some simple queries to check the data\n",
    "spark.sql(\"select sequences_id from sequences limit 10\").show()\n",
    "\n",
    "spark.sql(\"select a.keen_id, a.keen_timestamp, s.sequences_id from assessments a join sequences s on a.keen_id = s.keen_id limit 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|                  id|my_count|\n",
      "+--------------------+--------+\n",
      "|7a2ed6d3-f492-49b...|       1|\n",
      "|bbed4358-999d-446...|       2|\n",
      "|e6ad8644-96b1-461...|       3|\n",
      "|95194331-ac43-454...|       4|\n",
      "|95194331-ac43-454...|       1|\n",
      "|bbed4358-999d-446...|       2|\n",
      "|e6ad8644-96b1-461...|       3|\n",
      "|7a2ed6d3-f492-49b...|       4|\n",
      "|b9ff2e88-cf9d-4bd...|       1|\n",
      "|bec23e7b-4870-49f...|       2|\n",
      "+--------------------+--------+\n",
      "\n",
      "+--------------------+------------------+--------------------+\n",
      "|             keen_id|    keen_timestamp|                  id|\n",
      "+--------------------+------------------+--------------------+\n",
      "|5a17a67efa1257000...|1511499390.3836269|803fc93f-7eb2-412...|\n",
      "|5a17a67efa1257000...|1511499390.3836269|f3cb88cc-5b79-41b...|\n",
      "|5a17a67efa1257000...|1511499390.3836269|32fe7d8d-6d89-4db...|\n",
      "|5a17a67efa1257000...|1511499390.3836269|5c34cf19-8cfd-4f5...|\n",
      "|5a26ee9cbf5ce1000...|1512500892.4166169|0603e6f4-c3f9-4c2...|\n",
      "|5a26ee9cbf5ce1000...|1512500892.4166169|26a06b88-2758-45b...|\n",
      "|5a26ee9cbf5ce1000...|1512500892.4166169|25b6effe-79b0-4c4...|\n",
      "|5a26ee9cbf5ce1000...|1512500892.4166169|6de03a9b-2a78-46b...|\n",
      "|5a26ee9cbf5ce1000...|1512500892.4166169|aaf39991-fa83-470...|\n",
      "|5a26ee9cbf5ce1000...|1512500892.4166169|aab2e817-73dc-4ff...|\n",
      "+--------------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create another dataframe questions\n",
    "def my_lambda_questions(x):\n",
    "    raw_dict = json.loads(x.value)\n",
    "    my_list = []\n",
    "    my_count = 0\n",
    "    for l in raw_dict[\"sequences\"][\"questions\"]:\n",
    "        my_count += 1\n",
    "        my_dict = {\"keen_id\" : raw_dict[\"keen_id\"], \"my_count\" : my_count, \"id\" : l[\"id\"]}\n",
    "        my_list.append(Row(**my_dict))\n",
    "    return my_list\n",
    "\n",
    "my_questions = assessments.rdd.flatMap(my_lambda_questions).toDF()\n",
    "\n",
    "my_questions.registerTempTable('questions')\n",
    "\n",
    "spark.sql(\"select id, my_count from questions limit 10\").show()\n",
    "\n",
    "spark.sql(\"select q.keen_id, a.keen_timestamp, q.id from assessments a join questions q on a.keen_id = q.keen_id limit 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|correct|total|\n",
      "+-------+-----+\n",
      "|      2|    4|\n",
      "|      1|    4|\n",
      "|      3|    4|\n",
      "|      2|    4|\n",
      "|      3|    4|\n",
      "|      5|    5|\n",
      "|      1|    1|\n",
      "|      5|    5|\n",
      "|      4|    4|\n",
      "|      0|    5|\n",
      "+-------+-----+\n",
      "\n",
      "+-----+\n",
      "|score|\n",
      "+-----+\n",
      "|  0.5|\n",
      "| 0.25|\n",
      "| 0.75|\n",
      "|  0.5|\n",
      "| 0.75|\n",
      "|  1.0|\n",
      "|  1.0|\n",
      "|  1.0|\n",
      "|  1.0|\n",
      "|  0.0|\n",
      "+-----+\n",
      "\n",
      "+-----------------+\n",
      "|        avg_score|\n",
      "+-----------------+\n",
      "|62.65699745547047|\n",
      "+-----------------+\n",
      "\n",
      "+-------------------+\n",
      "| standard_deviation|\n",
      "+-------------------+\n",
      "|0.31086692286170553|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create another data frame correct total\n",
    "def my_lambda_correct_total(x):\n",
    "    \n",
    "    raw_dict = json.loads(x.value)\n",
    "    my_list = []\n",
    "    \n",
    "    if \"sequences\" in raw_dict:\n",
    "        \n",
    "        if \"counts\" in raw_dict[\"sequences\"]:\n",
    "            \n",
    "            if \"correct\" in raw_dict[\"sequences\"][\"counts\"] and \"total\" in raw_dict[\"sequences\"][\"counts\"]:\n",
    "                    \n",
    "                my_dict = {\"correct\": raw_dict[\"sequences\"][\"counts\"][\"correct\"], \n",
    "                           \"total\": raw_dict[\"sequences\"][\"counts\"][\"total\"]}\n",
    "                my_list.append(Row(**my_dict))\n",
    "    \n",
    "    return my_list\n",
    "\n",
    "my_correct_total = assessments.rdd.flatMap(my_lambda_correct_total).toDF()\n",
    "\n",
    "my_correct_total.registerTempTable('ct')\n",
    "\n",
    "# Some queries\n",
    "spark.sql(\"select * from ct limit 10\").show()\n",
    "\n",
    "spark.sql(\"select correct / total as score from ct limit 10\").show()\n",
    "\n",
    "spark.sql(\"select avg(correct / total)*100 as avg_score from ct limit 10\").show()\n",
    "\n",
    "spark.sql(\"select stddev(correct / total) as standard_deviation from ct limit 10\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (4) Answer Business questions\n",
    "The following questions can be answered with the queries offered after each question:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) What are the top 10 most taken classes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|           exam_name|number_of_students|\n",
      "+--------------------+------------------+\n",
      "|        Learning Git|               394|\n",
      "|Introduction to P...|               162|\n",
      "|Introduction to J...|               158|\n",
      "|Intermediate Pyth...|               158|\n",
      "|Learning to Progr...|               128|\n",
      "|Introduction to M...|               119|\n",
      "|Software Architec...|               109|\n",
      "|Beginning C# Prog...|                95|\n",
      "|    Learning Eclipse|                85|\n",
      "|Learning Apache M...|                80|\n",
      "+--------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explore exam names\n",
    "# spark.sql('select exam_name from assessments').show\n",
    "\n",
    "# Query for top 10 classes\n",
    "spark.sql('select exam_name, count(*) as number_of_students from assessments group by exam_name order by number_of_students desc').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) What are the bottom 10, least taken classes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|           exam_name|number_of_students|\n",
      "+--------------------+------------------+\n",
      "|Operating Red Hat...|                 1|\n",
      "|Native Web Apps f...|                 1|\n",
      "|Learning to Visua...|                 1|\n",
      "|Nulls, Three-valu...|                 1|\n",
      "|The Closed World ...|                 2|\n",
      "|Arduino Prototypi...|                 2|\n",
      "|What's New in Jav...|                 2|\n",
      "|Hibernate and JPA...|                 2|\n",
      "|Learning Spring P...|                 2|\n",
      "|Understanding the...|                 2|\n",
      "+--------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Bottom ten classes\n",
    "spark.sql('select exam_name, count(*) as number_of_students from assessments group by exam_name order by number_of_students asc').show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (5) Write to HDFS in parquet format for all created dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_assessments.write.mode('overwrite').parquet(\"/tmp/raw_assessments\")\n",
    "assessments.write.mode('overwrite').parquet(\"/tmp/assessments\")\n",
    "my_questions.write.mode('overwrite').parquet(\"/tmp/questions\")\n",
    "my_sequences.write.mode('overwrite').parquet(\"/tmp/sequences\")\n",
    "my_correct_total.write.mode('overwrite').parquet(\"/tmp/sequences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (6) Relevant history from the console"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "  952  cd ~/w205/project-2-jearcher/\n",
    "  953  docker-compose up -d\n",
    "  954  docker-compose logs -f kafka\n",
    "  955  docker-compose exec kafka kafka-topics --create --topic assessments --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181\n",
    "  956  docker-compose exec mids bash -c \"cat /w205/project-2-jearcher/assessment-attempts-20180128-121051-nested.json | jq '.[]' -c | kafkacat -P -b kafka:29092 -t assessments \"\n",
    "  957  docker-compose exec mids bash -c \"kafkacat -C -b kafka:29092 -t assessments -o beginning -e\"\n",
    "  958  history > josh-archer-history.txt\n",
    "  959  git status\n",
    "  960  git add .\n",
    "  961  git commit -m \"forgot to commit for a while, mostly finished with proj2\"\n",
    "  962  git push origin assignment\n",
    "  963  ls -lah\n",
    "  964  vi .gitignore\n",
    "  965  ls -lah\n",
    "  966  git add .\n",
    "  967  git commit -m \"adding to gitignore to only leave behind files for submission\"\n",
    "  968  git push origin assignment\n",
    "  969  git pull -all\n",
    "  970  git pull\n",
    "  971  git log\n",
    "  972  git status\n",
    "  973  ls -lah\n",
    "  974  rm .~derby.log\n",
    "  975  git status\n",
    "  976  git add .\n",
    "  977  git commit -m signing off to teach, nothing much added\"\n",
    "  978  git commit -m \"signing off to teach, nothing much added\"\n",
    "  979  git push origin assignment\n",
    "  980  history > josh-archer-history.txt\n",
    "  ```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
